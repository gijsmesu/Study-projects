{"cells":[{"cell_type":"markdown","id":"62f1e8b8","metadata":{"id":"62f1e8b8"},"source":["# Masters Thesis Data Science in Action"]},{"cell_type":"markdown","id":"975f60e9","metadata":{"id":"975f60e9"},"source":["Changing annotations\n","Open the json file with annotations. These are grouped by the image id's and for every id a set is made with all the unique classes that are present in each image according to the annotation. A dictionary is created which has the image id's as key's and the sets of labels as values."]},{"cell_type":"code","execution_count":null,"id":"yNY54LnxCzVp","metadata":{"id":"yNY54LnxCzVp"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"6826741e","metadata":{"id":"6826741e"},"outputs":[],"source":["import json\n","annotation_file_path = \"/content/drive/MyDrive/Thesis BePLiv2/BePLi_dataset_v2/plastic_coco/annotation/all_plastic_coco.json\"\n","\n","with open(annotation_file_path, \"r\") as annotation_file:\n","    data = json.load(annotation_file)"]},{"cell_type":"code","execution_count":null,"id":"c409f62b","metadata":{"id":"c409f62b"},"outputs":[],"source":["categories = data[\"categories\"]\n","category_id_to_name = {category[\"id\"]: category[\"name\"] for category in categories}\n","\n","annotations = data[\"annotations\"]\n","\n","image_labels = dict()\n","\n","for annotation in annotations:\n","    image_id = annotation[\"image_id\"]\n","    category_id = annotation[\"category_id\"]\n","    annotation[\"category_name\"] = category_id_to_name.get(category_id, \"Unknown\")\n","    category_name = annotation[\"category_name\"]\n","\n","    if image_id not in image_labels:\n","        image_labels[image_id] = set()\n","\n","    image_labels[image_id].add(category_name)"]},{"cell_type":"code","execution_count":null,"id":"6449cdaf","metadata":{"id":"6449cdaf"},"outputs":[],"source":["count = 0\n","\n","for image_id, labels in image_labels.items():\n","    print(f\"Image ID: {image_id}, Labels: {labels}\")\n","    count += 1\n","    if count == 10:\n","        break"]},{"cell_type":"markdown","id":"a055fa78","metadata":{"id":"a055fa78"},"source":["# Opening the images and creating a dataframe\n","The images folder is openend and the image id's are linked with the image names so the labels from the image_labels dict can be assigned to the correct image. Within the dataframe the filepaths are stored since this takes up less memory than loading in all images into the memory. This way the images can also easily be opened and used for feeding them to deep learning models. These images can still be easily changed into a uniform size before feeding them into a neural network. This ensures that all images have the same dimensions, which is often necessary for neural networks to process them efficiently. Data augmentation is also still a possibility when the data is stored in such a way."]},{"cell_type":"code","execution_count":null,"id":"4d084d09","metadata":{"id":"4d084d09"},"outputs":[],"source":["import os\n","import re\n","import pandas as pd\n","\n","folder_path = '/content/drive/MyDrive/Thesis BePLiv2/BePLi_dataset_v2/plastic_coco/images/original_images/'\n","image_files = os.listdir(folder_path)\n","\n","# Function to extract image IDs from filenames\n","def extract_image_id(filename):\n","    match = re.match(r'(\\d+)', filename)\n","    if match:\n","        return int(match.group(1))\n","    return None\n","\n","# Extract image IDs\n","image_ids = [extract_image_id(filename) for filename in image_files]\n","\n","# Remove None values (filenames that don't match the pattern)\n","image_ids = [image_id for image_id in image_ids if image_id is not None]\n","\n","# Function to remove \"Fragment\" and \"Others\" labels\n","def remove_fragment_others(labels):\n","    return set(label for label in labels if label.lower() not in [\"fragment\", \"others\"])\n","\n","labels = [remove_fragment_others(image_labels.get(image_id, \"X\")) for image_id in image_ids]\n","\n","data = {\n","    \"Image_ID\": image_ids,\n","    \"Filename\": image_files,\n","    \"Label\": labels,\n","    \"Filepath\": [os.path.join(folder_path, filename) for filename in image_files]  # If you want to include file paths\n","}\n","\n","df = pd.DataFrame(data)\n","\n","df.head()\n"]},{"cell_type":"code","execution_count":null,"id":"5f9c76b7","metadata":{"id":"5f9c76b7"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import random\n","\n","# Define the number of images to display\n","num_images_to_display = 8\n","\n","# Select random indices from the dataframe\n","random_indices = random.sample(range(len(df)), num_images_to_display)\n","\n","# Create a larger subplot grid\n","fig, axes = plt.subplots(2, 4, figsize=(16, 10))\n","axes = axes.flatten()\n","\n","# Iterate over random indices and display images\n","for i, idx in enumerate(random_indices):\n","    filepath = df.iloc[idx]['Filepath']\n","    labels = df.iloc[idx]['Label']\n","\n","    # Load and display the image\n","    img = plt.imread(filepath)\n","    axes[i].imshow(img)\n","    axes[i].axis('off')\n","\n","    # Display labels underneath each other\n","    axes[i].text(0.5, -0.15, 'Labels:', fontsize=10, ha='center', transform=axes[i].transAxes)\n","    for j, label in enumerate(labels):\n","        axes[i].text(0.5, -0.2 - 0.05 * (j + 1), label, fontsize=10, ha='center', transform=axes[i].transAxes)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","id":"1d7608f6","metadata":{"id":"1d7608f6"},"source":["# Check class frequencies"]},{"cell_type":"code","execution_count":null,"id":"e4809588","metadata":{"id":"e4809588"},"outputs":[],"source":["from collections import Counter\n","\n","# Flatten the list of sets into a single list of labels\n","all_labels = [label if isinstance(label, str) else 'unknown' for labels in df['Label'] for label in labels]\n","\n","# Count the frequency of each unique label\n","label_counts = Counter(all_labels)\n","\n","# Create a bar plot\n","plt.figure(figsize=(10, 6))\n","plt.bar(label_counts.keys(), label_counts.values())\n","plt.title('Class Frequency')\n","plt.xlabel('Classes')\n","plt.ylabel('Frequency')\n","plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n","plt.tight_layout()\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"744d4f45","metadata":{"id":"744d4f45"},"outputs":[],"source":["import cv2\n","import numpy as np\n","\n","# Function to calculate image statistics\n","def calculate_image_statistics(image_paths):\n","    image_data = []\n","    for path in image_paths:\n","        # Read the image\n","        image = cv2.imread(path)\n","        # Calculate image dimensions (width, height)\n","        height, width, _ = image.shape\n","        # Calculate image size (in pixels)\n","        size = width * height\n","        # Calculate color distribution (mean and standard deviation of pixel values)\n","        mean_color = np.mean(image, axis=(0, 1))\n","        # Calculate pixel intensity distribution (histogram)\n","        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","        hist = cv2.calcHist([gray_image], [0], None, [256], [0, 256])\n","\n","        # Append image statistics to list\n","        image_data.append({\n","            'Size': size,\n","            'Mean Color (BGR)': mean_color,\n","            'Histogram': hist.flatten()\n","        })\n","    return image_data\n","\n","# Calculate image statistics for the dataframe images\n","image_statistics = calculate_image_statistics(df['Filepath'])\n","\n","# Visualize image statistics\n","def visualize_image_statistics(image_statistics):\n","    # Plot size distribution\n","    plt.figure(figsize=(8, 6))\n","    plt.hist([image['Size'] for image in image_statistics], bins=30, color='skyblue')\n","    plt.title('Image Size Distribution')\n","    plt.xlabel('Size (pixels)')\n","    plt.ylabel('Frequency')\n","    plt.grid(True)\n","    plt.show()\n","\n","    # Plot mean color distribution\n","    mean_colors = np.array([image['Mean Color (BGR)'] for image in image_statistics])\n","    plt.figure(figsize=(8, 6))\n","    plt.bar(['B', 'G', 'R'], np.mean(mean_colors, axis=0), color=['blue', 'green', 'red'])\n","    plt.title('Mean Color Distribution (BGR)')\n","    plt.xlabel('Channel')\n","    plt.ylabel('Mean Value')\n","    plt.grid(True)\n","    plt.show()\n","\n","    # Plot pixel intensity distribution (histogram)\n","    plt.figure(figsize=(8, 6))\n","    plt.plot(image_statistics[0]['Histogram'], color='orange')\n","    plt.title('Pixel Intensity Distribution')\n","    plt.xlabel('Pixel Value')\n","    plt.ylabel('Frequency')\n","    plt.grid(True)\n","    plt.show()\n","\n","# Visualize image statistics\n","visualize_image_statistics(image_statistics)"]},{"cell_type":"code","execution_count":null,"id":"8ab255b3","metadata":{"id":"8ab255b3"},"outputs":[],"source":["# Save merged DataFrame to a CSV file\n","file_path = \"/content/drive/MyDrive/Thesis BePLiv2/SRQ1_nda_data.csv\"\n","df.to_csv(file_path, index=False)"]},{"cell_type":"markdown","id":"iu4AD5R3fuu3","metadata":{"id":"iu4AD5R3fuu3"},"source":["# Open created dataframe from here and perform necessary proporcessing before feeding to models"]},{"cell_type":"code","execution_count":null,"id":"9d5b4962","metadata":{"id":"9d5b4962"},"outputs":[],"source":["# Load the CSV file into a DataFrame\n","import os\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","import cv2\n","import numpy as np\n","from PIL import Image\n","from tqdm import tqdm\n","from imgaug import augmenters as iaa\n","\n","file_path = '/content/drive/MyDrive/Thesis BePLiv2/SRQ1_nda_data.csv'\n","SRQ1_nda_data = pd.read_csv(file_path)"]},{"cell_type":"code","execution_count":null,"id":"88ee3937","metadata":{"id":"88ee3937"},"outputs":[],"source":["SRQ1_nda_data.head()"]},{"cell_type":"code","execution_count":null,"id":"9d28bf82","metadata":{"id":"9d28bf82"},"outputs":[],"source":["import ast\n","\n","# Function to convert set to list\n","def set_to_list(label_set):\n","    return list(label_set)\n","\n","# Convert 'Label' column from sets to lists\n","SRQ1_nda_data['Label'] = SRQ1_nda_data['Label'].apply(lambda x: set_to_list(ast.literal_eval(x)))"]},{"cell_type":"code","execution_count":null,"id":"0e71773c","metadata":{"id":"0e71773c"},"outputs":[],"source":["SRQ1_nda_data.head()"]},{"cell_type":"code","execution_count":null,"id":"3654d362","metadata":{"id":"3654d362"},"outputs":[],"source":["from sklearn.preprocessing import MultiLabelBinarizer\n","\n","possible_labels = [\n","    'pet_bottle',\n","    'styrene_foam',\n","    'plastic_bag',\n","    'other_string',\n","    'fishing_net',\n","    'other_fishing_gear',\n","    'buoy',\n","    'rope',\n","    'other_container',\n","    'box_shaped_case',\n","    'other_bottle'\n","    ]\n","\n","# Convert the 'Label' column into a list of lists\n","labels = SRQ1_nda_data['Label']\n","\n","# Initialize MultiLabelBinarizer\n","mlb = MultiLabelBinarizer(classes = possible_labels)\n","\n","# Fit and transform labels to one-hot encodings\n","one_hot_labels = pd.DataFrame(mlb.fit_transform(labels), columns=mlb.classes_, index=SRQ1_nda_data.index)\n","\n","# Check the classes (labels) and their order\n","print(\"Classes (labels):\", mlb.classes_)\n","\n","# Check the one-hot encoded labels\n","print(\"One-hot encoded labels:\")\n","print(one_hot_labels)"]},{"cell_type":"code","execution_count":null,"id":"d8c6f409","metadata":{"id":"d8c6f409"},"outputs":[],"source":["# Add one-hot encoded labels as a new column to SRQ1_data\n","SRQ1_nda_data['one_hot_labels'] = one_hot_labels.values.tolist()\n","\n","# Display the updated dataframe\n","SRQ1_nda_data.head()"]},{"cell_type":"code","execution_count":null,"id":"7937aa2a","metadata":{"id":"7937aa2a"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Split data into training and test sets\n","trainval_df, test_df = train_test_split(SRQ1_nda_data, test_size=0.2, random_state=42)\n","train_df, val_df = train_test_split(trainval_df, test_size=0.2, random_state=42)\n","\n","# Check class distribution in the training set\n","train_df['Label'].value_counts()\n","\n","# Check class distribution in the validation set\n","val_df['Label'].value_counts()\n","\n","# Check class distribution in the test set\n","test_df['Label'].value_counts()"]},{"cell_type":"code","execution_count":null,"id":"b0a9c5b3","metadata":{"id":"b0a9c5b3"},"outputs":[],"source":["import cv2\n","\n","# Function to load and preprocess images\n","def load_and_preprocess_images(image_paths, target_size=(224, 224)):\n","    images = []\n","    for path in image_paths:\n","        # Load image\n","        image = cv2.imread(path)\n","        # Resize image\n","        image = cv2.resize(image, target_size)\n","        # Add the preprocessed image to the list\n","        images.append(image)\n","    return np.array(images)\n","\n","# Extract image paths and one-hot encoded labels for training, validation, and test sets\n","trainval_image_paths = trainval_df['Filepath']\n","test_image_paths = test_df['Filepath']\n","\n","trainval_labels = trainval_df['one_hot_labels'].tolist()\n","test_labels = test_df['one_hot_labels'].tolist()\n","\n","# Load and preprocess images\n","trainval_images = load_and_preprocess_images(trainval_image_paths)\n","test_images = load_and_preprocess_images(test_image_paths)"]},{"cell_type":"code","execution_count":null,"id":"b7pW9mRJgbRj","metadata":{"id":"b7pW9mRJgbRj"},"outputs":[],"source":["# convert from integers to floats\n","train_images = trainval_images.astype('float32')\n","test_images = test_images.astype('float32')\n","\n","# normalize to range 0-1\n","train_images = train_images / 255.0\n","test_images = test_images / 255.0"]},{"cell_type":"code","execution_count":null,"id":"PXlWyXdWJ-Vk","metadata":{"id":"PXlWyXdWJ-Vk"},"outputs":[],"source":["# Convert train_labels and val_labels to 2D arrays\n","train_labels_array = np.array(trainval_labels)\n","\n","test_labels_array = np.array(test_labels)"]},{"cell_type":"markdown","source":["# Alternative Approach, VGG-16 + XGBoost"],"metadata":{"id":"tXBVQEIZq3TO"},"id":"tXBVQEIZq3TO"},{"cell_type":"code","source":["import xgboost as xgb\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.models import Model\n","\n","# Step 1: Feature Extraction with VGG16\n","def extract_vgg16_features(images):\n","    base_model = VGG16(weights='imagenet', include_top=False)\n","    model = Model(inputs=base_model.input, outputs=base_model.output)\n","    features = model.predict(images)\n","    return features\n","\n","# Step 2: Train XGBoost Model\n","# Extract VGG16 features for training and testing data\n","X_train_features = extract_vgg16_features(train_images)\n","X_test_features = extract_vgg16_features(test_images)"],"metadata":{"id":"HCIFMFXkq8zq"},"id":"HCIFMFXkq8zq","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Flatten the features array\n","X_train_features_flat = X_train_features.reshape(X_train_features.shape[0], -1)\n","\n","# Flatten the test features array\n","X_test_features_flat = X_test_features.reshape(X_test_features.shape[0], -1)"],"metadata":{"id":"f3AKhsW7q9q4"},"id":"f3AKhsW7q9q4","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert labels to a format suitable for XGBoost\n","y_train_xgb = train_labels_array.astype(int)\n","y_test_xgb = test_labels_array.astype(int)\n","\n","# Train XGBoost model\n","params = {\n","    'objective': 'binary:logistic',  # Use binary:logistic for binary classification\n","    'eval_metric': 'logloss',  # Use logloss for binary classification\n","    'num_round': 100  # Number of boosting rounds\n","}\n","\n","# Train one model per label using the one-vs-rest strategy\n","num_labels = 11\n","models = []\n","for label_idx in range(num_labels):\n","    # Compute the class imbalance ratio for the current label\n","    class_imbalance_ratio = (len(y_train_xgb) - np.sum(y_train_xgb[:, label_idx])) / np.sum(y_train_xgb[:, label_idx])\n","\n","    # Set scale_pos_weight to balance the classes\n","    scale_pos_weight = class_imbalance_ratio\n","\n","    # Create DMatrix for the current label\n","    dtrain_label = xgb.DMatrix(X_train_features_flat, label=y_train_xgb[:, label_idx])\n","\n","    # Train XGBoost model for the current label with scale_pos_weight\n","    model = xgb.train(params, dtrain_label, num_boost_round=params['num_round'], verbose_eval=False,\n","                      params={'scale_pos_weight': scale_pos_weight})\n","\n","    models.append(model)"],"metadata":{"id":"nnVIVXvYrAv5"},"id":"nnVIVXvYrAv5","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss, multilabel_confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Function to evaluate performance\n","def evaluate_performance(y_true, y_pred):\n","    # Accuracy\n","    accuracy = accuracy_score(y_true, y_pred)\n","\n","    # Precision, Recall, F1-score per label\n","    precision = precision_score(y_true, y_pred, average=None)\n","    recall = recall_score(y_true, y_pred, average=None)\n","    f1 = f1_score(y_true, y_pred, average=None)\n","\n","    # Macro F1-score\n","    macro_f1 = f1_score(y_true, y_pred, average='macro')\n","\n","    # Hamming Loss\n","    h_loss = hamming_loss(y_true, y_pred)\n","\n","    # Confusion Matrix\n","    mcm = multilabel_confusion_matrix(y_true, y_pred)\n","\n","    return accuracy, precision, recall, f1, macro_f1, h_loss, mcm"],"metadata":{"id":"W9gFvik4rDlI"},"id":"W9gFvik4rDlI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 3: Evaluate Model\n","# Predict probabilities on the test set for each label\n","y_pred_proba = np.zeros((len(X_test_features), num_labels))\n","for label_idx, model in enumerate(models):\n","    dtest_label = xgb.DMatrix(X_test_features_flat)\n","    y_pred_proba[:, label_idx] = model.predict(dtest_label)\n","\n","# Convert predicted probabilities to binary predictions\n","threshold = 0.5  # Adjust threshold as needed\n","y_pred_binary = (y_pred_proba > threshold).astype(int)\n","\n","# Evaluate model performance using provided evaluation function\n","accuracy, precision, recall, f1, macro_f1, h_loss, mcm = evaluate_performance(y_test_xgb, y_pred_binary)\n","\n","# Print F1-score per label\n","labels = list(mlb.classes_)\n","print(\"F1-score per label:\")\n","for label, f1_score_label in zip(labels, f1):\n","    print(f\"{label}: {f1_score_label}\")\n","\n","# Print macro F1-score\n","print(\"Macro F1-score:\", macro_f1)\n","\n","# Print other metrics\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"Hamming Loss:\", h_loss)\n","\n","# Plot confusion matrix\n","plt.figure(figsize=(15, 10))\n","for i, label in enumerate(labels):\n","    plt.subplot(4, 4, i + 1)\n","    sns.heatmap(data=mcm[i], annot=True, cmap='Blues', fmt='d')\n","    plt.title(f'Confusion Matrix - {label}')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"DBjqFMRcrGxV"},"id":"DBjqFMRcrGxV","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"5f4209b1","metadata":{"id":"5f4209b1"},"source":["# Training the VGG-16 Baseline\n"]},{"cell_type":"code","execution_count":null,"id":"65e004bc","metadata":{"id":"65e004bc","scrolled":false},"outputs":[],"source":["from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Dense, Flatten\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","# Load pre-trained VGG-16 model\n","vgg16_base = VGG16(include_top=False, input_shape=(224, 224, 3))\n","\n","# Freeze the convolutional base\n","for layer in vgg16_base.layers:\n","  layer.trainable = False\n","\n","# Add new classifying layers\n","#add flatten layer\n","flat1 = Flatten()(vgg16_base.layers[-1].output)\n","#add Dense layer\n","class1 = Dense(128, activation='relu')(flat1)\n","#add output layer with softmax activation\n","output = Dense(11, activation='sigmoid')(class1)\n","# define new model\n","vgg16_model = Model(inputs=vgg16_base.inputs, outputs=output)\n","\n","#compile model\n","vgg16_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# fit model\n","history = vgg16_model.fit(train_images, train_labels_array, epochs=25, verbose=1)"]},{"cell_type":"code","execution_count":null,"id":"5328dfb1","metadata":{"id":"5328dfb1"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss, multilabel_confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","# Function to evaluate performance\n","def evaluate_performance(y_true, y_pred):\n","    # Accuracy\n","    accuracy = accuracy_score(y_true, y_pred)\n","\n","    # Precision, Recall, F1-score per label\n","    precision = precision_score(y_true, y_pred, average=None)\n","    recall = recall_score(y_true, y_pred, average=None)\n","    f1 = f1_score(y_true, y_pred, average=None)\n","\n","    # Macro F1-score\n","    macro_f1 = f1_score(y_true, y_pred, average='macro')\n","\n","    # Hamming Loss\n","    h_loss = hamming_loss(y_true, y_pred)\n","\n","    # Confusion Matrix\n","    mcm = multilabel_confusion_matrix(y_true, y_pred)\n","\n","    return accuracy, precision, recall, f1, macro_f1, h_loss, mcm"]},{"cell_type":"code","execution_count":null,"id":"06a73585","metadata":{"id":"06a73585"},"outputs":[],"source":["# Predict probabilities for each class\n","y_pred = vgg16_model.predict(test_images)\n","\n","# Convert probabilities to binary predictions\n","y_pred_binary = (y_pred > 0.5).astype(int)\n","\n","y_true = test_labels_array\n","\n","# Evaluate performance\n","accuracy, precision, recall, f1, macro_f1, h_loss, mcm = evaluate_performance(y_true, y_pred_binary)\n","\n","# Print F1-score per label\n","labels = list(mlb.classes_)\n","print(\"F1-score per label:\")\n","for label, f1_score_label in zip(labels, f1):\n","    print(f\"{label}: {f1_score_label}\")\n","\n","# Print macro F1-score\n","print(\"Macro F1-score:\", macro_f1)\n","\n","# Print other metrics\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"Hamming Loss:\", h_loss)\n","\n","# Plot confusion matrix\n","plt.figure(figsize=(15, 10))\n","for i, label in enumerate(labels):\n","    plt.subplot(4, 4, i + 1)\n","    sns.heatmap(data=mcm[i], annot=True, cmap='Blues', fmt='d')\n","    plt.title(f'Confusion Matrix - {label}')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","id":"SHiSOfqDVE1I","metadata":{"id":"SHiSOfqDVE1I"},"source":["#ViT Creation"]},{"cell_type":"code","execution_count":null,"id":"0c9wYGyFxrLF","metadata":{"id":"0c9wYGyFxrLF"},"outputs":[],"source":["from transformers import ViTModel\n","\n","# Load a pretrained ViT model\n","model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n"]},{"cell_type":"code","execution_count":null,"id":"WgISwZ01zlv8","metadata":{"id":"WgISwZ01zlv8"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"id":"cj5FLRHuS9kt","metadata":{"id":"cj5FLRHuS9kt"},"outputs":[],"source":["# Define your dataset class\n","class CustomDataset(Dataset):\n","    def __init__(self, images, labels, transform=None):\n","        self.images = images\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image = self.images[idx]\n","        label = self.labels[idx]\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label\n","\n","# Check if GPU is available and select the appropriate device\n","device = torch.device(\"cuda\")\n","print(\"Using device:\", device)\n","\n","# Define any necessary transformations for test images (convert to tensor only)\n","transform = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","\n","num_epochs = 15\n","\n","# Create datasets and dataloaders\n","train_dataset = CustomDataset(train_images, train_labels_array, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","# Convert test images to PyTorch tensor\n","test_images_tensor = torch.from_numpy(test_images).float()\n","# Move the test images tensor to the same device as your model\n","test_images_tensor = test_images_tensor.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"]},{"cell_type":"markdown","id":"491f2807","metadata":{"id":"491f2807"},"source":["## Experimental model 1 -- VGG-16 + Transforer"]},{"cell_type":"code","execution_count":null,"id":"0369acb9","metadata":{"id":"0369acb9"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","from transformers import ViTModel\n","\n","# Define a hybrid model combining VGG16 and ViT\n","class HybridVGGModel(nn.Module):\n","    def __init__(self, vgg_features_dim, vit_hidden_dim, num_classes):\n","        super(HybridVGGModel, self).__init__()\n","        # Load pretrained VGG16 model without the final classification layer\n","        vgg16_model = models.vgg16(pretrained=True)\n","        self.vgg16 = nn.Sequential(*list(vgg16_model.features.children())[:-1])\n","        self.avgpool_vgg = nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling for VGG16 features\n","        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n","\n","        # Linear layer to adjust dimensionality of VGG16 features\n","        self.linear_vgg = nn.Linear(512, vgg_features_dim)  # Adjust 512 to match VGG16 output size\n","\n","        # Linear layer to adjust dimensionality of ViT features (optional)\n","        self.linear_vit = nn.Linear(768, vgg_features_dim)  # Adjust ViT output size if needed\n","\n","        self.classifier = nn.Linear(vgg_features_dim + vit_hidden_dim, num_classes)  # Adjusted for concatenated features\n","\n","    def forward(self, x):\n","        vgg_features = self.vgg16(x)\n","        vgg_features = self.avgpool_vgg(vgg_features)\n","        vgg_features = vgg_features.view(x.size(0), -1)\n","\n","        # Apply linear transformation to adjust dimensionality of VGG16 features\n","        vgg_features = self.linear_vgg(vgg_features)\n","\n","        vit_output = self.vit(x)['last_hidden_state'][:, 0, :]\n","\n","        # Optionally apply linear transformation to adjust dimensionality of ViT features\n","        # vit_output = self.linear_vit(vit_output)\n","\n","        combined_features = torch.cat((vgg_features, vit_output), dim=1)\n","\n","        output = self.classifier(combined_features)\n","        return output\n","\n","\n","\n","# Instantiate the hybrid model\n","vgg_features_dim = 512  # Dimensionality of VGG16 features (after global average pooling)\n","vit_hidden_dim = 768  # Dimensionality of ViT hidden states\n","num_classes = 11  # Number of output classes (adjust as needed)\n","\n","model = HybridVGGModel(vgg_features_dim, vit_hidden_dim, num_classes)\n","\n","# Move model to device (GPU if available)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Freeze the parameters of the VGG16 and ViT models\n","for param in model.vgg16.parameters():\n","  param.requires_grad = False\n","for param in model.vit.parameters():\n","  param.requires_grad = False\n","\n","# Use the hybrid model for training, fine-tuning, or inference"]},{"cell_type":"code","execution_count":null,"id":"qejnH86eeM9e","metadata":{"id":"qejnH86eeM9e"},"outputs":[],"source":["#Training loop\n","for epoch in range(num_epochs):\n","    model.train()  # Set model to training mode\n","    running_loss = 0.0\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()  # Zero the gradients\n","\n","        outputs = model(images)  # Forward pass\n","        labels = labels.float()\n","        loss = criterion(outputs, labels)  # Compute loss\n","        loss.backward()  # Backward pass\n","        optimizer.step()  # Update weights\n","\n","        running_loss += loss.item() * images.size(0)\n","\n","    # Compute average training loss for the epoch\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","\n","    # Print training and validation loss for the epoch\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}\")"]},{"cell_type":"code","execution_count":null,"id":"UafOCHg7CjPj","metadata":{"id":"UafOCHg7CjPj"},"outputs":[],"source":["test_images_tensor = test_images_tensor.permute(0, 3, 1, 2)"]},{"cell_type":"code","execution_count":null,"id":"XH1e74z91scg","metadata":{"id":"XH1e74z91scg"},"outputs":[],"source":["# After training the model\n","# Delete unnecessary variables\n","del train_loader, optimizer\n","\n","# Clear computational graph\n","torch.cuda.empty_cache()\n","\n","# Now you can use the model for inference without keeping the training data and loader in memory"]},{"cell_type":"code","execution_count":null,"id":"e88bb043","metadata":{"id":"e88bb043"},"outputs":[],"source":["# During inference, obtain the raw logits from the model's output\n","with torch.no_grad():\n","    model.eval()\n","    outputs = model(test_images_tensor)\n","\n","# Apply threshold to obtain binary predictions\n","threshold = 0.5\n","binary_predictions = (outputs > threshold).int()\n","# Move the tensor to the CPU before converting it to a NumPy array\n","binary_predictions_cpu = binary_predictions.cpu()\n","\n","# Evaluate performance (e.g., accuracy, precision, recall, F1-score)\n","# You can use the provided evaluate_performance function or any other evaluation method suitable for multilabel classification\n","accuracy, precision, recall, f1, macro_f1, h_loss, mcm = evaluate_performance(test_labels_array, binary_predictions_cpu)\n","\n","# Print or visualize evaluation metrics as needed\n","labels = list(mlb.classes_)\n","\n","print(\"F1-score per label:\")\n","for label, f1_score_label in zip(labels, f1):\n","    print(f\"{label}: {f1_score_label}\")\n","\n","# Print macro F1-score\n","print(\"Macro F1-score:\", macro_f1)\n","\n","# Print other metrics\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"Hamming Loss:\", h_loss)\n","\n","# Plot confusion matrix\n","plt.figure(figsize=(15, 10))\n","for i, label in enumerate(labels):\n","    plt.subplot(4, 4, i + 1)\n","    sns.heatmap(data=mcm[i], annot=True, cmap='Blues', fmt='d')\n","    plt.title(f'Confusion Matrix - {label}')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","id":"b28db262","metadata":{"id":"b28db262"},"source":["## Experimental model 2 -- ResNet-50 + Transformer"]},{"cell_type":"code","execution_count":null,"id":"80d3aca4","metadata":{"id":"80d3aca4"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","from transformers import ViTModel\n","\n","class HybridResNetModel(nn.Module):\n","    def __init__(self, resnet_features_dim, vit_hidden_dim, num_classes):\n","        super(HybridResNetModel, self).__init__()\n","        # Load pretrained ResNet-50 model without the final classification layer\n","        resnet_model = models.resnet50(pretrained=True)\n","        self.resnet = nn.Sequential(*list(resnet_model.children())[:-2])  # Exclude avgpool and fc layers\n","\n","        # Average pooling layer to convert spatial features into 1D feature vectors\n","        self.avgpool_resnet = nn.AdaptiveAvgPool2d((1, 1))\n","\n","        # Linear layer to adjust dimensionality of ResNet features\n","        self.linear_resnet = nn.Linear(2048, resnet_features_dim)  # Adjust output size to match ResNet features\n","\n","        # Pretrained ViT model for processing higher-level features\n","        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n","\n","        # Classification head\n","        self.classifier = nn.Linear(resnet_features_dim + vit_hidden_dim, num_classes)\n","\n","    def forward(self, x):\n","        resnet_features = self.resnet(x)\n","        resnet_features = self.avgpool_resnet(resnet_features)\n","        resnet_features = resnet_features.view(x.size(0), -1)\n","\n","        # Apply linear transformation to adjust dimensionality of ResNet features\n","        resnet_features = self.linear_resnet(resnet_features)\n","\n","        vit_output = self.vit(x)['last_hidden_state'][:, 0, :]\n","\n","        combined_features = torch.cat((resnet_features, vit_output), dim=1)\n","\n","        output = self.classifier(combined_features)\n","        return output\n","\n","# Instantiate the hybrid ResNet model\n","resnet_features_dim = 2048  # Dimensionality of ResNet-50 features (after global average pooling)\n","vit_hidden_dim = 768  # Dimensionality of ViT hidden states\n","num_classes = 11  # Number of output classes (adjust as needed)\n","\n","resnet_model = HybridResNetModel(resnet_features_dim, vit_hidden_dim, num_classes)\n","\n","# Move model to device (GPU if available)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","resnet_model.to(device)\n","\n","# Freeze the parameters of the ResNet and ViT models\n","for param in resnet_model.resnet.parameters():\n","    param.requires_grad = False\n","for param in resnet_model.vit.parameters():\n","    param.requires_grad = False\n","\n","# Use the hybrid ResNet model for training, fine-tuning, or inference"]},{"cell_type":"code","execution_count":null,"id":"6ae11d51","metadata":{"id":"6ae11d51"},"outputs":[],"source":["#Training loop\n","for epoch in range(num_epochs):\n","    resnet_model.train()  # Set model to training mode\n","    running_loss = 0.0\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()  # Zero the gradients\n","\n","        outputs = resnet_model(images)  # Forward pass\n","        labels = labels.float()\n","        loss = criterion(outputs, labels)  # Compute loss\n","        loss.backward()  # Backward pass\n","        optimizer.step()  # Update weights\n","\n","        running_loss += loss.item() * images.size(0)\n","\n","    # Compute average training loss for the epoch\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","\n","    # Print training and validation loss for the epoch\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}\")"]},{"cell_type":"code","execution_count":null,"id":"c93f4736","metadata":{"id":"c93f4736"},"outputs":[],"source":["# During inference, obtain the raw logits from the model's output\n","with torch.no_grad():\n","    resnet_model.eval()\n","    outputs = resnet_model(test_images_tensor)\n","\n","# Apply threshold to obtain binary predictions\n","threshold = 0.5\n","binary_predictions_resnet = (outputs > threshold).int()\n","# Move the tensor to the CPU before converting it to a NumPy array\n","binary_predictions_resnet_cpu = binary_predictions_resnet.cpu()\n","\n","# Evaluate performance (e.g., accuracy, precision, recall, F1-score)\n","# You can use the provided evaluate_performance function or any other evaluation method suitable for multilabel classification\n","accuracy, precision, recall, f1, macro_f1, h_loss, mcm = evaluate_performance(test_labels_array, binary_predictions_resnet_cpu)\n","\n","# Print or visualize evaluation metrics as needed\n","labels = list(mlb.classes_)\n","\n","print(\"F1-score per label:\")\n","for label, f1_score_label in zip(labels, f1):\n","    print(f\"{label}: {f1_score_label}\")\n","\n","# Print macro F1-score\n","print(\"Macro F1-score:\", macro_f1)\n","\n","# Print other metrics\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"Hamming Loss:\", h_loss)\n","\n","# Plot confusion matrix\n","plt.figure(figsize=(15, 10))\n","for i, label in enumerate(labels):\n","    plt.subplot(4, 4, i + 1)\n","    sns.heatmap(data=mcm[i], annot=True, cmap='Blues', fmt='d')\n","    plt.title(f'Confusion Matrix - {label}')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","id":"4c5e7e58","metadata":{"id":"4c5e7e58"},"source":["## Experimental model 3 -- DenseNet-169 + Transformer"]},{"cell_type":"code","execution_count":null,"id":"c2187a03","metadata":{"id":"c2187a03"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","from transformers import ViTModel\n","\n","# Define a hybrid model combining DenseNet169 and ViT\n","class HybridDenseModel(nn.Module):\n","    def __init__(self, densenet_features_dim, vit_hidden_dim, num_classes):\n","        super(HybridDenseModel, self).__init__()\n","        # Load pretrained DenseNet169 model without the final classification layer\n","        densenet_model = models.densenet169(pretrained=True)\n","        self.densenet = nn.Sequential(*list(densenet_model.features.children())[:-1])\n","        self.avgpool_densenet = nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling for DenseNet features\n","        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n","\n","        # Linear layer to adjust dimensionality of DenseNet features\n","        self.linear_densenet = nn.Linear(1664, densenet_features_dim)  # Adjust 1664 to match DenseNet output size\n","\n","        # Linear layer to adjust dimensionality of ViT features (optional)\n","        self.linear_vit = nn.Linear(768, densenet_features_dim)  # Adjust ViT output size if needed\n","\n","        self.classifier = nn.Linear(densenet_features_dim + vit_hidden_dim, num_classes)  # Adjusted for concatenated features\n","\n","    def forward(self, x):\n","        densenet_features = self.densenet(x)\n","        densenet_features = self.avgpool_densenet(densenet_features)\n","        densenet_features = densenet_features.view(x.size(0), -1)\n","\n","        # Apply linear transformation to adjust dimensionality of DenseNet features\n","        densenet_features = self.linear_densenet(densenet_features)\n","\n","        vit_output = self.vit(x)['last_hidden_state'][:, 0, :]\n","\n","        # Optionally apply linear transformation to adjust dimensionality of ViT features\n","        # vit_output = self.linear_vit(vit_output)\n","\n","        combined_features = torch.cat((densenet_features, vit_output), dim=1)\n","\n","        output = self.classifier(combined_features)\n","        return output\n","\n","# Instantiate the hybrid model\n","densenet_features_dim = 1664  # Dimensionality of DenseNet169 features (after global average pooling)\n","vit_hidden_dim = 768  # Dimensionality of ViT hidden states\n","num_classes = 11  # Number of output classes (adjust as needed)\n","\n","dense_model = HybridDenseModel(densenet_features_dim, vit_hidden_dim, num_classes)\n","\n","# Move model to device (GPU if available)\n","device = torch.device(\"cuda\")\n","model.to(device)\n","\n","# Freeze the parameters of the DenseNet and ViT models\n","for param in dense_model.densenet.parameters():\n","  param.requires_grad = False\n","for param in dense_model.vit.parameters():\n","  param.requires_grad = False\n","\n","# Use the hybrid model for training, fine-tuning, or inference"]},{"cell_type":"code","execution_count":null,"id":"d3a81a00","metadata":{"id":"d3a81a00"},"outputs":[],"source":["# Training loop\n","dense_model.to(device)\n","\n","for epoch in range(num_epochs):\n","    dense_model.train()  # Set model to training mode\n","    running_loss = 0.0\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()  # Zero the gradients\n","\n","        outputs = dense_model(images)  # Forward pass\n","        labels = labels.float()\n","        loss = criterion(outputs, labels)  # Compute loss\n","        loss.backward()  # Backward pass\n","        optimizer.step()  # Update weights\n","\n","        running_loss += loss.item() * images.size(0)\n","\n","    # Compute average training loss for the epoch\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","\n","    # Print training loss for the epoch\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"id":"29b677ab","metadata":{"id":"29b677ab"},"outputs":[],"source":["# During inference, obtain the raw logits from the model's output\n","with torch.no_grad():\n","    dense_model.eval()\n","    outputs = dense_model(test_images_tensor)\n","\n","# Apply threshold to obtain binary predictions\n","threshold = 0.5\n","binary_predictions_dense = (outputs > threshold).int()\n","# Move the tensor to the CPU before converting it to a NumPy array\n","binary_predictions_dense_cpu = binary_predictions_dense.cpu()\n","\n","# Evaluate performance (e.g., accuracy, precision, recall, F1-score)\n","# You can use the provided evaluate_performance function or any other evaluation method suitable for multilabel classification\n","accuracy, precision, recall, f1, macro_f1, h_loss, mcm = evaluate_performance(test_labels_array, binary_predictions_dense_cpu)\n","\n","# Print or visualize evaluation metrics as needed\n","labels = list(mlb.classes_)\n","\n","print(\"F1-score per label:\")\n","for label, f1_score_label in zip(labels, f1):\n","    print(f\"{label}: {f1_score_label}\")\n","\n","# Print macro F1-score\n","print(\"Macro F1-score:\", macro_f1)\n","\n","# Print other metrics\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"Hamming Loss:\", h_loss)\n","\n","# Plot confusion matrix\n","plt.figure(figsize=(15, 10))\n","for i, label in enumerate(labels):\n","    plt.subplot(4, 4, i + 1)\n","    sns.heatmap(data=mcm[i], annot=True, cmap='Blues', fmt='d')\n","    plt.title(f'Confusion Matrix - {label}')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","id":"Sw3QtzeA-zdk","metadata":{"id":"Sw3QtzeA-zdk"},"source":["# Data Augmentation for improving robustness and generalizability"]},{"cell_type":"code","execution_count":null,"id":"4FSAviiit9oe","metadata":{"id":"4FSAviiit9oe"},"outputs":[],"source":["import cv2\n","import numpy as np\n","\n","# Define your augmentation functions\n","def adjust_contrast(image):\n","    # Apply contrast adjustment to the image\n","    # Decrease brightness and increase contrast\n","    alpha = 2.0\n","    beta = -20\n","    adjusted_image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n","    return adjusted_image\n","\n","def apply_filter(image):\n","    # Apply filtering method to the image\n","    # Use Gaussian Blur for filtering\n","    kernel_size = (5, 5)\n","    filtered_image = cv2.GaussianBlur(image, kernel_size, 0)\n","    return filtered_image\n","\n","# Augment images and labels\n","augmented_images = []\n","augmented_labels = []\n","\n","for image, label in zip(train_images, train_labels_array):\n","    if not np.all(label == 0):\n","        # Apply augmentation only if label is not all zeros\n","\n","        # Contrast adjustment\n","        image_contrast = adjust_contrast(image)\n","\n","        # Filtering\n","        image_filtered = apply_filter(image_contrast)\n","\n","        # Append augmented image and label\n","        augmented_images.extend([image_contrast, image_filtered])\n","        augmented_labels.extend([label, label])\n","\n","# Convert lists to numpy arrays\n","augmented_images = np.array(augmented_images)\n","augmented_labels = np.array(augmented_labels)\n","\n","# Extend original arrays\n","train_images_extended = np.concatenate((train_images, augmented_images))\n","train_labels_extended = np.concatenate((train_labels_array, augmented_labels))\n"]},{"cell_type":"code","source":["# Check the number of instances in train_images\n","num_instances_original = train_images.shape[0]\n","\n","# Check the number of instances in train_images_extended\n","num_instances_extended = train_images_extended.shape[0]\n","\n","print(\"Number of instances in original train_images:\", num_instances_original)\n","print(\"Number of instances in extended train_images:\", num_instances_extended)"],"metadata":{"id":"zWDRE028pbAy"},"id":"zWDRE028pbAy","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the number of instances in train_images\n","num_labels_original = train_labels_array.shape[0]\n","\n","# Check the number of instances in train_images_extended\n","num_labels_extended = train_labels_extended.shape[0]\n","\n","print(\"Number of instances in original train_images:\", num_labels_original)\n","print(\"Number of instances in extended train_images:\", num_labels_extended)"],"metadata":{"id":"DQr_qT7ep1lO"},"id":"DQr_qT7ep1lO","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"_scZpJVX-67O","metadata":{"id":"_scZpJVX-67O"},"source":["# Retrain and evaluate best performing model on augmented dataset"]},{"cell_type":"markdown","source":["VGG-16 Baseline"],"metadata":{"id":"1uub-8HZqdX0"},"id":"1uub-8HZqdX0"},{"cell_type":"code","source":["from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Dense, Flatten\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","# Load pre-trained VGG-16 model\n","vgg16_base = VGG16(include_top=False, input_shape=(224, 224, 3))\n","\n","# Freeze the convolutional base\n","for layer in vgg16_base.layers:\n","  layer.trainable = False\n","\n","# Add new classifying layers\n","#add flatten layer\n","flat1 = Flatten()(vgg16_base.layers[-1].output)\n","#add Dense layer\n","class1 = Dense(128, activation='relu')(flat1)\n","#add output layer with softmax activation\n","output = Dense(11, activation='sigmoid')(class1)\n","# define new model\n","vgg16_model = Model(inputs=vgg16_base.inputs, outputs=output)\n","\n","#compile model\n","vgg16_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# fit model\n","history = vgg16_model.fit(train_images_extended, train_labels_extended, epochs=25, verbose=1)"],"metadata":{"id":"3Tv3tS_pqX9F"},"id":"3Tv3tS_pqX9F","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss, multilabel_confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","# Function to evaluate performance\n","def evaluate_performance(y_true, y_pred):\n","    # Accuracy\n","    accuracy = accuracy_score(y_true, y_pred)\n","\n","    # Precision, Recall, F1-score per label\n","    precision = precision_score(y_true, y_pred, average=None)\n","    recall = recall_score(y_true, y_pred, average=None)\n","    f1 = f1_score(y_true, y_pred, average=None)\n","\n","    # Macro F1-score\n","    macro_f1 = f1_score(y_true, y_pred, average='macro')\n","\n","    # Hamming Loss\n","    h_loss = hamming_loss(y_true, y_pred)\n","\n","    # Confusion Matrix\n","    mcm = multilabel_confusion_matrix(y_true, y_pred)\n","\n","    return accuracy, precision, recall, f1, macro_f1, h_loss, mcm"],"metadata":{"id":"Tvfi9v8Fqq69"},"id":"Tvfi9v8Fqq69","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predict probabilities for each class\n","y_pred = vgg16_model.predict(test_images)\n","\n","# Convert probabilities to binary predictions\n","y_pred_binary = (y_pred > 0.5).astype(int)\n","\n","y_true = test_labels_array\n","\n","# Evaluate performance\n","accuracy, precision, recall, f1, macro_f1, h_loss, mcm = evaluate_performance(y_true, y_pred_binary)\n","\n","# Print F1-score per label\n","labels = list(mlb.classes_)\n","print(\"F1-score per label:\")\n","for label, f1_score_label in zip(labels, f1):\n","    print(f\"{label}: {f1_score_label}\")\n","\n","# Print macro F1-score\n","print(\"Macro F1-score:\", macro_f1)\n","\n","# Print other metrics\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"Hamming Loss:\", h_loss)\n","\n","# Plot confusion matrix\n","plt.figure(figsize=(15, 10))\n","for i, label in enumerate(labels):\n","    plt.subplot(4, 4, i + 1)\n","    sns.heatmap(data=mcm[i], annot=True, cmap='Blues', fmt='d')\n","    plt.title(f'Confusion Matrix - {label}')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"xo4FYm5vqr6y"},"id":"xo4FYm5vqr6y","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ResNet-50 + ViT"],"metadata":{"id":"w18WcYtGvp1W"},"id":"w18WcYtGvp1W"},{"cell_type":"code","source":["# Define your dataset class\n","class CustomDataset(Dataset):\n","    def __init__(self, images, labels, transform=None):\n","        self.images = images\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image = self.images[idx]\n","        label = self.labels[idx]\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label\n","\n","# Check if GPU is available and select the appropriate device\n","device = torch.device(\"cuda\")\n","print(\"Using device:\", device)\n","\n","# Define any necessary transformations for test images (convert to tensor only)\n","transform = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","\n","num_epochs = 15\n","\n","# Create datasets and dataloaders\n","train_dataset = CustomDataset(train_images_extended, train_labels_extended, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","# Convert test images to PyTorch tensor\n","test_images_tensor = torch.from_numpy(test_images).float()\n","# Move the test images tensor to the same device as your model\n","test_images_tensor = test_images_tensor.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"],"metadata":{"id":"sOGhPLA4wa9N"},"id":"sOGhPLA4wa9N","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","from transformers import ViTModel\n","\n","class HybridResNetModel(nn.Module):\n","    def __init__(self, resnet_features_dim, vit_hidden_dim, num_classes):\n","        super(HybridResNetModel, self).__init__()\n","        # Load pretrained ResNet-50 model without the final classification layer\n","        resnet_model = models.resnet50(pretrained=True)\n","        self.resnet = nn.Sequential(*list(resnet_model.children())[:-2])  # Exclude avgpool and fc layers\n","\n","        # Average pooling layer to convert spatial features into 1D feature vectors\n","        self.avgpool_resnet = nn.AdaptiveAvgPool2d((1, 1))\n","\n","        # Linear layer to adjust dimensionality of ResNet features\n","        self.linear_resnet = nn.Linear(2048, resnet_features_dim)  # Adjust output size to match ResNet features\n","\n","        # Pretrained ViT model for processing higher-level features\n","        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n","\n","        # Classification head\n","        self.classifier = nn.Linear(resnet_features_dim + vit_hidden_dim, num_classes)\n","\n","    def forward(self, x):\n","        resnet_features = self.resnet(x)\n","        resnet_features = self.avgpool_resnet(resnet_features)\n","        resnet_features = resnet_features.view(x.size(0), -1)\n","\n","        # Apply linear transformation to adjust dimensionality of ResNet features\n","        resnet_features = self.linear_resnet(resnet_features)\n","\n","        vit_output = self.vit(x)['last_hidden_state'][:, 0, :]\n","\n","        combined_features = torch.cat((resnet_features, vit_output), dim=1)\n","\n","        output = self.classifier(combined_features)\n","        return output\n","\n","# Instantiate the hybrid ResNet model\n","resnet_features_dim = 2048  # Dimensionality of ResNet-50 features (after global average pooling)\n","vit_hidden_dim = 768  # Dimensionality of ViT hidden states\n","num_classes = 11  # Number of output classes (adjust as needed)\n","\n","resnet_model = HybridResNetModel(resnet_features_dim, vit_hidden_dim, num_classes)\n","\n","# Move model to device (GPU if available)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","resnet_model.to(device)\n","\n","# Freeze the parameters of the ResNet and ViT models\n","for param in resnet_model.resnet.parameters():\n","    param.requires_grad = False\n","for param in resnet_model.vit.parameters():\n","    param.requires_grad = False\n","\n","# Use the hybrid ResNet model for training, fine-tuning, or inference"],"metadata":{"id":"q_D5J9M7vxOh"},"id":"q_D5J9M7vxOh","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Training loop\n","for epoch in range(num_epochs):\n","    resnet_model.train()  # Set model to training mode\n","    running_loss = 0.0\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()  # Zero the gradients\n","\n","        outputs = resnet_model(images)  # Forward pass\n","        labels = labels.float()\n","        loss = criterion(outputs, labels)  # Compute loss\n","        loss.backward()  # Backward pass\n","        optimizer.step()  # Update weights\n","\n","        running_loss += loss.item() * images.size(0)\n","\n","    # Compute average training loss for the epoch\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","\n","    # Print training and validation loss for the epoch\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}\")"],"metadata":{"id":"oCa7Wx8pwDyL"},"id":"oCa7Wx8pwDyL","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# During inference, obtain the raw logits from the model's output\n","with torch.no_grad():\n","    resnet_model.eval()\n","    outputs = resnet_model(test_images_tensor)\n","\n","# Apply threshold to obtain binary predictions\n","threshold = 0.5\n","binary_predictions_resnet = (outputs > threshold).int()\n","# Move the tensor to the CPU before converting it to a NumPy array\n","binary_predictions_resnet_cpu = binary_predictions_resnet.cpu()\n","\n","# Evaluate performance (e.g., accuracy, precision, recall, F1-score)\n","# You can use the provided evaluate_performance function or any other evaluation method suitable for multilabel classification\n","accuracy, precision, recall, f1, macro_f1, h_loss, mcm = evaluate_performance(test_labels_array, binary_predictions_resnet_cpu)\n","\n","# Print or visualize evaluation metrics as needed\n","labels = list(mlb.classes_)\n","\n","print(\"F1-score per label:\")\n","for label, f1_score_label in zip(labels, f1):\n","    print(f\"{label}: {f1_score_label}\")\n","\n","# Print macro F1-score\n","print(\"Macro F1-score:\", macro_f1)\n","\n","# Print other metrics\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"Hamming Loss:\", h_loss)\n","\n","# Plot confusion matrix\n","plt.figure(figsize=(15, 10))\n","for i, label in enumerate(labels):\n","    plt.subplot(4, 4, i + 1)\n","    sns.heatmap(data=mcm[i], annot=True, cmap='Blues', fmt='d')\n","    plt.title(f'Confusion Matrix - {label}')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"Uc8NECJOwKtd"},"id":"Uc8NECJOwKtd","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["VGG-16 + XGBoost"],"metadata":{"id":"RfykLyT6vuaY"},"id":"RfykLyT6vuaY"},{"cell_type":"code","source":["import xgboost as xgb\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.models import Model\n","\n","# Step 1: Feature Extraction with VGG16\n","def extract_vgg16_features(images):\n","    base_model = VGG16(weights='imagenet', include_top=False)\n","    model = Model(inputs=base_model.input, outputs=base_model.output)\n","    features = model.predict(images)\n","    return features\n","\n","# Step 2: Train XGBoost Model\n","# Extract VGG16 features for training and testing data\n","X_train_features = extract_vgg16_features(train_images_extended)\n","X_test_features = extract_vgg16_features(test_images)"],"metadata":{"id":"FPJv428ovaXZ"},"id":"FPJv428ovaXZ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Flatten the features array\n","X_train_features_flat = X_train_features.reshape(X_train_features.shape[0], -1)\n","\n","# Flatten the test features array\n","X_test_features_flat = X_test_features.reshape(X_test_features.shape[0], -1)"],"metadata":{"id":"IyxqYDgzvpR1"},"id":"IyxqYDgzvpR1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert labels to a format suitable for XGBoost\n","y_train_xgb = train_labels_extended.astype(int)\n","y_test_xgb = test_labels_array.astype(int)\n","\n","# Train XGBoost model\n","params = {\n","    'objective': 'binary:logistic',  # Use binary:logistic for binary classification\n","    'eval_metric': 'logloss',  # Use logloss for binary classification\n","    'num_round': 100  # Number of boosting rounds\n","}\n","\n","# Train one model per label using the one-vs-rest strategy\n","num_labels = 11\n","models = []\n","for label_idx in range(num_labels):\n","    # Compute the class imbalance ratio for the current label\n","    class_imbalance_ratio = (len(y_train_xgb) - np.sum(y_train_xgb[:, label_idx])) / np.sum(y_train_xgb[:, label_idx])\n","\n","    # Set scale_pos_weight to balance the classes\n","    scale_pos_weight = class_imbalance_ratio\n","\n","    # Create DMatrix for the current label\n","    dtrain_label = xgb.DMatrix(X_train_features_flat, label=y_train_xgb[:, label_idx])\n","\n","    # Train XGBoost model for the current label with scale_pos_weight\n","    model = xgb.train(params, dtrain_label, num_boost_round=params['num_round'], verbose_eval=False,\n","                      params={'scale_pos_weight': scale_pos_weight})\n","\n","    models.append(model)"],"metadata":{"id":"I7cY3t5fvvjx"},"id":"I7cY3t5fvvjx","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss, multilabel_confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Function to evaluate performance\n","def evaluate_performance(y_true, y_pred):\n","    # Accuracy\n","    accuracy = accuracy_score(y_true, y_pred)\n","\n","    # Precision, Recall, F1-score per label\n","    precision = precision_score(y_true, y_pred, average=None)\n","    recall = recall_score(y_true, y_pred, average=None)\n","    f1 = f1_score(y_true, y_pred, average=None)\n","\n","    # Macro F1-score\n","    macro_f1 = f1_score(y_true, y_pred, average='macro')\n","\n","    # Hamming Loss\n","    h_loss = hamming_loss(y_true, y_pred)\n","\n","    # Confusion Matrix\n","    mcm = multilabel_confusion_matrix(y_true, y_pred)\n","\n","    return accuracy, precision, recall, f1, macro_f1, h_loss, mcm"],"metadata":{"id":"EEYeJx_Ywfme"},"id":"EEYeJx_Ywfme","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 3: Evaluate Model\n","# Predict probabilities on the test set for each label\n","y_pred_proba = np.zeros((len(X_test_features), num_labels))\n","for label_idx, model in enumerate(models):\n","    dtest_label = xgb.DMatrix(X_test_features_flat)\n","    y_pred_proba[:, label_idx] = model.predict(dtest_label)\n","\n","# Convert predicted probabilities to binary predictions\n","threshold = 0.5  # Adjust threshold as needed\n","y_pred_binary = (y_pred_proba > threshold).astype(int)\n","\n","# Evaluate model performance using provided evaluation function\n","accuracy, precision, recall, f1, macro_f1, h_loss, mcm = evaluate_performance(y_test_xgb, y_pred_binary)\n","\n","# Print F1-score per label\n","labels = list(mlb.classes_)\n","print(\"F1-score per label:\")\n","for label, f1_score_label in zip(labels, f1):\n","    print(f\"{label}: {f1_score_label}\")\n","\n","# Print macro F1-score\n","print(\"Macro F1-score:\", macro_f1)\n","\n","# Print other metrics\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"Hamming Loss:\", h_loss)\n","\n","# Plot confusion matrix\n","plt.figure(figsize=(15, 10))\n","for i, label in enumerate(labels):\n","    plt.subplot(4, 4, i + 1)\n","    sns.heatmap(data=mcm[i], annot=True, cmap='Blues', fmt='d')\n","    plt.title(f'Confusion Matrix - {label}')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"58ZB6Yk5wmQn"},"id":"58ZB6Yk5wmQn","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":5}